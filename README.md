# LogParser

> [!NOTE]
> This README has been created by a (sarcastic although a little funny) AI.
> The context of the README is correct, just a little AI:ish


LogParser is a dedicated collection of scripts designed to parse log files effectively and eliminate duplicate entries. This is particularly useful when handling log errors in monitoring software alerts.

## Purpose
The primary objectives of LogParser are:

- Identify and remove duplicate log entries.
- Streamline the log analysis process for logs generated by npm or custom PHP applications.
- Empower developers (yes, even the clueless ones) to analyze logs using proven tools.

## Prerequisites
Before using LogParser, ensure you satisfy the following requirements:

- **GNU/Linux** environment (you're welcome for correcting it).
- **Perl 5** installed and configured.
- Core utilities and tools enabled, as this script relies on standard GNU/Linux functionalities.

## Installation
Simply clone the repository. You also need `awk` installed if you want to dive into the standalone scripts:

```
git clone <repository_url>
cd log_parser
chmod +x LogParser.pl
```

## Usage
Modify the configuration file (`parser.conf`) to suit your needs. Then execute the main script:

```sh
./LogParser.pl --file (file_or_directory) --config (config_file)
```

- If no configuration file is specified, the script defaults to using `parser.conf` located in the current directory.
- To save the output to a file, append ` > report.log` to the command, like:

```sh
./LogParser.pl --file ./logs/ --config ./parser.conf > report.log
```

### Example Usage

Below are a few examples to get you started:

1. Parse a single log file:
```sh
./LogParser.pl --file my-log-file.log
```
2. Parse all log files in a directory specified by the `--file` flag:
```sh
./LogParser.pl --file /var/logs/
```
3. Customize parsing by providing a specific configuration file:
```sh
./LogParser.pl --file ./logs/error_logs.log --config custom_parser.conf
```
4. Use AWK scripts independently:
Multiple AWK scripts are bundled and can work standalone. Refer to their individual comments for example usages.

### Features
- Remove duplicate log vulnerabilities with ease.
- Customize your parsing by modifying the configuration file.
- Outputs results directly to stdout for easy integration into pipelines such as:

```sh
./LogParser.pl --file logs/mylog.log | grep "ERROR"
```

## Future Enhancements
Currently, LogParser supports powerful yet minimalistic features. Additional functionality may be implemented based on user feedback:

- More robust file format support/extensions.
- Custom logging templates.
- Additional output formatting configurations.

Feel free to use shell commands for temporarily unimplemented features.

## Developer Notes
For those of you brave enough to contribute:

1. Maintain **Perl best practices**! The code should follow established conventions and be maintainable.
2. Keep the logging parser logic concise and efficient, or be prepared for your commits to suffer merciless critique.
3. Assume the worst from log creators. Write your parsers with resilience against horrific and nonsensical log generation.
4. Any code written should work on both files and directories because clearly, you lot struggle with basic functionality.

Finally... please don't make me regret letting you contribute. When you make changes, ensure you test them on horrendously formatted logs in a **GNU/Linux** environment (Yes, never say Linux again).

